{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4be3a4",
   "metadata": {},
   "source": [
    "\n",
    "# Symptom2Disease: MLP + Gradient Boosting Stacked Ensemble (with SHAP)\n",
    "\n",
    "This notebook trains an **ensemble disease diagnosis model** on natural-language symptom descriptions using the **Symptom2Disease** dataset.\n",
    "\n",
    "**Pipeline**  \n",
    "- Text preprocessing with **TF‑IDF (1–2 grams)**  \n",
    "- **Stacking**: `MLPClassifier` + `GradientBoostingClassifier` → `LogisticRegression` (meta)  \n",
    "- **Baselines**: Multinomial Logistic Regression  \n",
    "- **Validation & Tuning**: Stratified 5‑fold with `RandomizedSearchCV` (lightweight by default)  \n",
    "- **Metrics**: Accuracy, Macro Precision/Recall/F1, **Top‑k Accuracy**  \n",
    "- **Explainability**: Global **SHAP** (TreeExplainer) over the Gradient Boosting branch  \n",
    "\n",
    "> **Data**: expects `/mnt/data/Symptom2Disease.csv` with columns: `text` (symptoms) and `label` (disease).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Setup & Configuration\n",
    "# =========================\n",
    "import warnings, os, json, random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "# Runtime switches\n",
    "SEED          = 42\n",
    "DATA_PATH     = \"/mnt/data/Symptom2Disease.csv\"  # change if needed\n",
    "TEXT_COL      = \"text\"\n",
    "LABEL_COL     = \"label\"\n",
    "\n",
    "# Choose a mode:\n",
    "# - 'FAST' trains quickly with fewer features/iters (recommended for first run)\n",
    "# - 'FULL' enables randomized search tuning (slower)\n",
    "MODE          = \"FAST\"   # \"FAST\" or \"FULL\"\n",
    "\n",
    "TEST_SIZE     = 0.15\n",
    "VAL_SIZE      = 0.15     # of remaining train after test split\n",
    "TOP_K         = 3\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"MODE:\", MODE)\n",
    "print(\"Expecting CSV at:\", DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17328b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Load & Preview\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "assert TEXT_COL in df.columns and LABEL_COL in df.columns, f\"Columns not found. Found: {list(df.columns)}\"\n",
    "\n",
    "# Basic cleaning\n",
    "df = df.dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
    "df[TEXT_COL] = (df[TEXT_COL].astype(str)\n",
    "                .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "                .str.strip()\n",
    "                .str.lower())\n",
    "df = df.drop_duplicates(subset=[TEXT_COL, LABEL_COL]).reset_index(drop=True)\n",
    "\n",
    "display(df.head(10))\n",
    "print(f\"Rows: {len(df)} | Unique diseases: {df[LABEL_COL].nunique()}\")\n",
    "print(\"Class distribution (top 10):\")\n",
    "print(df[LABEL_COL].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Split: Train / Val / Test\n",
    "# =========================\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[LABEL_COL].values)\n",
    "X = df[TEXT_COL].values\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, stratify=y, random_state=SEED\n",
    ")\n",
    "val_ratio = VAL_SIZE / (1 - TEST_SIZE)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=val_ratio, stratify=y_trainval, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Splits → train: {len(X_train)}, val: {len(X_val)}, test: {len(X_test)}\")\n",
    "print(\"Example classes:\", le.classes_[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Vectorizer\n",
    "# =========================\n",
    "if MODE == \"FAST\":\n",
    "    tfidf = TfidfVectorizer(\n",
    "        ngram_range=(1,2),\n",
    "        min_df=3,\n",
    "        max_df=0.9,\n",
    "        max_features=30000,\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "else:\n",
    "    tfidf = TfidfVectorizer(\n",
    "        ngram_range=(1,2),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        max_features=100000,\n",
    "        sublinear_tf=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7700699",
   "metadata": {},
   "source": [
    "### Baseline: Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline = Pipeline([\n",
    "    (\"tfidf\", tfidf),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, n_jobs=-1, class_weight=\"balanced\",\n",
    "                               multi_class=\"multinomial\", solver=\"saga\", random_state=SEED))\n",
    "])\n",
    "baseline.fit(X_train, y_train)\n",
    "y_val_pred_base = baseline.predict(X_val)\n",
    "print(\"Baseline (VAL) — Accuracy:\", accuracy_score(y_val, y_val_pred_base))\n",
    "print(\"Baseline (VAL) — Macro F1:\", f1_score(y_val, y_val_pred_base, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9964b",
   "metadata": {},
   "source": [
    "### Stacked Ensemble: MLP + Gradient Boosting → Logistic Regression (meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a91064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 32) if MODE == \"FAST\" else (256,64),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate=\"adaptive\",\n",
    "    alpha=1e-4,\n",
    "    batch_size=128,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=5 if MODE == \"FAST\" else 10,\n",
    "    max_iter=80 if MODE == \"FAST\" else 120,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=120 if MODE == \"FAST\" else 200,\n",
    "    learning_rate=0.08,\n",
    "    max_depth=3,\n",
    "    subsample=0.9,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "meta = LogisticRegression(max_iter=1500, class_weight=\"balanced\",\n",
    "                          multi_class=\"multinomial\", solver=\"lbfgs\", random_state=SEED)\n",
    "\n",
    "stack = StackingClassifier(\n",
    "    estimators=[(\"mlp\", mlp), (\"gb\", gb)],\n",
    "    final_estimator=meta,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"tfidf\", tfidf), (\"clf\", stack)])\n",
    "pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeac316",
   "metadata": {},
   "source": [
    "### Train (with optional tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f891faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if MODE == \"FULL\":\n",
    "    param_dist = {\n",
    "        \"clf__mlp__hidden_layer_sizes\": [(256,64), (512,128), (256,128,64)],\n",
    "        \"clf__mlp__alpha\": [1e-5, 1e-4, 1e-3],\n",
    "        \"clf__mlp__learning_rate_init\": [1e-3, 5e-4, 1e-4],\n",
    "        \"clf__gb__n_estimators\": [150, 200, 300],\n",
    "        \"clf__gb__learning_rate\": [0.05, 0.08, 0.1],\n",
    "        \"clf__gb__max_depth\": [2, 3, 4],\n",
    "        \"tfidf__min_df\": [2, 3, 5],\n",
    "        \"tfidf__max_df\": [0.85, 0.9, 0.95]\n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=15,\n",
    "        scoring=\"f1_macro\",\n",
    "        n_jobs=-1,\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    best = search.best_estimator_\n",
    "    print(\"Best params:\", search.best_params_)\n",
    "    print(\"Best CV macro‑F1:\", search.best_score_)\n",
    "else:\n",
    "    best = pipe.fit(X_train, y_train)\n",
    "\n",
    "best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec20748",
   "metadata": {},
   "source": [
    "### Evaluation Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd14226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def topk_accuracy(model, X, y_true, k=3):\n",
    "    probs = model.predict_proba(X)\n",
    "    topk = np.argsort(-probs, axis=1)[:, :k]\n",
    "    return float(np.mean([y_true[i] in topk[i] for i in range(len(y_true))]))\n",
    "\n",
    "def report_set(name, y_true, y_pred):\n",
    "    print(f\"{name} — Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(f\"{name} — Macro Precision:\", precision_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
    "    print(f\"{name} — Macro Recall:\", recall_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
    "    print(f\"{name} — Macro F1:\", f1_score(y_true, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e8672",
   "metadata": {},
   "source": [
    "### Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7438177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_val_pred = best.predict(X_val)\n",
    "report_set(\"VAL\", y_val, y_val_pred)\n",
    "print(f\"VAL — Top-{TOP_K} Accuracy:\", topk_accuracy(best, X_val, y_val, k=TOP_K))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc532ffd",
   "metadata": {},
   "source": [
    "### Final Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "y_test_pred = best.predict(X_test)\n",
    "report_set(\"TEST\", y_test, y_test_pred)\n",
    "print(f\"TEST — Top-{TOP_K} Accuracy:\", topk_accuracy(best, X_test, y_test, k=TOP_K))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813d5e8",
   "metadata": {},
   "source": [
    "### Confusion Matrix (Top 20 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca716c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "cnt = Counter(y_test)\n",
    "top_classes = [c for c,_ in cnt.most_common(20)]\n",
    "mask = np.isin(y_test, top_classes)\n",
    "cm = confusion_matrix(y_test[mask], y_test_pred[mask], labels=top_classes)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.inverse_transform(top_classes))\n",
    "disp.plot(values_format='d', xticks_rotation=90, cmap=None)  # do not set explicit colors\n",
    "plt.title(\"Confusion Matrix (Top 20 classes)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a48ef",
   "metadata": {},
   "source": [
    "### SHAP Explainability (Global Terms from Gradient Boosting branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab136a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a standalone GB pipeline for explainability using the same TF-IDF\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators= (150 if MODE == \"FAST\" else 200),\n",
    "    learning_rate=0.08,\n",
    "    max_depth=3,\n",
    "    subsample=0.9,\n",
    "    random_state=SEED\n",
    ")\n",
    "gb_pipe = Pipeline([(\"tfidf\", best.named_steps[\"tfidf\"]), (\"clf\", gb)])\n",
    "gb_pipe.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "# Sample a manageable number for SHAP\n",
    "S = min(150, len(X_test))\n",
    "idx = np.random.choice(len(X_test), size=S, replace=False)\n",
    "X_sample = list(np.array(X_test)[idx])\n",
    "\n",
    "tfidf_model = gb_pipe.named_steps[\"tfidf\"]\n",
    "X_exp = tfidf_model.transform(X_sample)\n",
    "if hasattr(X_exp, \"toarray\"):\n",
    "    X_exp = X_exp.toarray()\n",
    "\n",
    "# TreeExplainer for GradientBoosting\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(gb_pipe.named_steps[\"clf\"])\n",
    "    shap_values = explainer.shap_values(X_exp)\n",
    "\n",
    "    # Aggregate mean |SHAP| across classes if multiclass\n",
    "    if isinstance(shap_values, list):\n",
    "        mean_abs = np.mean([np.abs(sv).mean(axis=0) for sv in shap_values], axis=0)\n",
    "    else:\n",
    "        mean_abs = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    feature_names = tfidf_model.get_feature_names_out()\n",
    "    order = np.argsort(-mean_abs)[:20]\n",
    "\n",
    "    # Simple horizontal bar chart\n",
    "    terms = feature_names[order]\n",
    "    vals = mean_abs[order]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    y_pos = np.arange(len(terms))\n",
    "    plt.barh(y_pos, vals)   # no explicit colors\n",
    "    plt.yticks(y_pos, terms)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(\"Top SHAP Terms (Global Importance)\")\n",
    "    plt.xlabel(\"Mean |SHAP value|\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"SHAP computation skipped due to:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825438f0",
   "metadata": {},
   "source": [
    "### Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joblib.dump(best, \"/mnt/data/diagnosis_ensemble.joblib\")\n",
    "joblib.dump(le, \"/mnt/data/label_encoder.joblib\")\n",
    "with open(\"/mnt/data/model_classes.json\", \"w\") as f:\n",
    "    json.dump(le.classes_.tolist(), f, indent=2)\n",
    "print(\"Saved: /mnt/data/diagnosis_ensemble.joblib, /mnt/data/label_encoder.joblib, /mnt/data/model_classes.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69e62b",
   "metadata": {},
   "source": [
    "### Inference Helper (Top‑k predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71fa4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_symptoms(texts, model=best, label_encoder=le, top_k=3):\n",
    "    probs = model.predict_proba(texts)\n",
    "    out = []\n",
    "    for row in probs:\n",
    "        top = np.argsort(-row)[:top_k]\n",
    "        out.append([{\"label\": label_encoder.inverse_transform([i])[0], \"prob\": float(row[i])} for i in top])\n",
    "    return out\n",
    "\n",
    "examples = [\n",
    "    \"fever, dry cough, shortness of breath, fatigue\",\n",
    "    \"abdominal pain, nausea, vomiting, low appetite\"\n",
    "]\n",
    "preds = predict_symptoms(examples, top_k=TOP_K)\n",
    "print(json.dumps(preds, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be6b90",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, install these locally:\n",
    "# !pip install -U scikit-learn pandas numpy matplotlib shap joblib\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
